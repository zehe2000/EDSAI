{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5beddcfb",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "Elements of Data Science and Artificial Intelligence\n",
    "<br>\n",
    "Winter Semester 2022/2023\n",
    "<br>\n",
    "Saarland University\n",
    "\n",
    "Prof. Dr. Dittrich\n",
    "<br>\n",
    "Prof. Dr. Hoffmann\n",
    "<br>\n",
    "Prof. Dr. Schiele\n",
    "<br>\n",
    "Dr. Schuster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e2a88",
   "metadata": {},
   "source": [
    "The first assignment is exclusively about Python and asks you to solve small tasks. Insert your solution in the cells marked with `TODO: implement`. For each exercise, we have written a few unit tests so you are able to check your implementation right away (keep in mind that passing the unit tests does not necessarily mean your implementation computes the desired output for all possible inputs of your function!). In case you have questions, feel free to use our [forum](https://edsai.cs.uni-saarland.de/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d342da",
   "metadata": {},
   "source": [
    "**You are not allowed to use third-party libraries or additional Python modules unless otherwise specified.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f6d2f",
   "metadata": {},
   "source": [
    "#### Submission Details\n",
    "Upload your submission to our [CMS](https://cms.sic.saarland/edsai2223/) in groups of two to three students until **November 23, 2022 23:59**. Late submissions will not be graded! Your submission should only contain this **.ipynb** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eb064757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the unittests\n",
    "%run -i assignment01_unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f2ea3",
   "metadata": {},
   "source": [
    "## Exercise 1: Statistical computations (4 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd9537",
   "metadata": {},
   "source": [
    "### a) Arithmetic mean (1 Points)\n",
    "Implement a function `mean(input_list)` that returns the arithmetic mean of the elements contained in `input_list`. The arithmetic mean is defined as follows:\n",
    "$$\n",
    "\\text{mean}([a_{0}, \\cdots, a_{n-1}]) = \\frac{1}{n}\\sum_{i=0}^{n-1} a_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b9a98d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(X):\n",
    "    \"\"\"\n",
    "    This function calculates the mean of X.\n",
    "    Args:\n",
    "        X (list): list of numbers.\n",
    "    \"\"\"\n",
    "    return sum(X)/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dfc77295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_large (__main__.MeanTest) ... ok\n",
      "test_negative (__main__.MeanTest) ... ok\n",
      "test_same (__main__.MeanTest) ... ok\n",
      "test_small (__main__.MeanTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x204e633fcd0>"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tests\n",
    "# Note: due to floating point precision, we test for almost equality\n",
    "# see: https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertAlmostEqual\n",
    "unittest.main(argv=['ignored', '-v', 'MeanTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000dcee",
   "metadata": {},
   "source": [
    "### b) Variance (1.5 Points)\n",
    "Implement a function `variance(input_list)` that returns the variance of the elements contained in *input_list*. The variance is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{variance}([a_{0}, \\cdots, a_{n-1}]) = \\frac{\\sum_{i=0}^{n-1} (a_{i}-\\mu)^2}{n - 1}\n",
    "$$\n",
    "where:\n",
    "* $[a_{0}, \\cdots, a_{n-1}]$ is a list of integers\n",
    "* $\\mu$ is the mean value of the elements in the list\n",
    "* $n$ is the number of elements in the list\n",
    "\n",
    "You may assume that *input_list* contains at least two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "da9cd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance (X):\n",
    "    var_i=0\n",
    "    for a_i in X:\n",
    "        var_i+=(a_i-mean(X))**2\n",
    "\n",
    "    return var_i/(len(X)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e5c86530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_large (__main__.VarTest) ... ok\n",
      "test_negative (__main__.VarTest) ... ok\n",
      "test_same (__main__.VarTest) ... ok\n",
      "test_small (__main__.VarTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x204e633fd00>"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tests\n",
    "# Note: due to floating point precision, we test for almost equality\n",
    "# see: https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertAlmostEqual\n",
    "unittest.main(argv=['ignored', '-v', 'VarTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0f10a",
   "metadata": {},
   "source": [
    "### c) Median (1.5 Points)\n",
    "Implement a function `median(input_list)` that returns the median of a given *input_list*. The median is the value separating the lower half from the upper half of a dataset and is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{median}([a_{0}, \\cdots, a_{n-1}]) =\n",
    "\\begin{cases}\n",
    "a_{\\frac{n-1}{2}}, & n \\text{ is odd}\\\\\n",
    "\\frac{1}{2} (a_{\\frac{n}{2}-1} + a_{\\frac{n}{2}})  & n \\text{ is even}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $[a_{0}, \\cdots, a_{n-1}]$ is a sorted list of integers and $n$ is the number of elements in the list.\n",
    "\n",
    "You may assume that *input_list* is not empty but not necessarily sorted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ebcfdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median (X):\n",
    "    X.sort()\n",
    "    if len(X)%2==0:\n",
    "        return (X[int(len(X)/2)] + X[int(len(X)/2-1)])/2.0\n",
    "    else:\n",
    "        return X[int((len(X))/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95993fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_even (__main__.MedianTest) ... ok\n",
      "test_odd (__main__.MedianTest) ... ok\n",
      "test_single (__main__.MedianTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x204e63a2fa0>"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tests\n",
    "unittest.main(argv=['ignored', '-v', 'MedianTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f33a7",
   "metadata": {},
   "source": [
    "## Exercise 2: Complex Functions (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e846eca",
   "metadata": {},
   "source": [
    "### a) Sum of the digits (2 Points)\n",
    "Implement a function `sum_digits(x)` that returns the sum of the digits of a non-negative integer $x$. For example:\n",
    "\n",
    "$$ \\text{sum_digits(1374)} = 1 + 3 + 7 + 4 = 15 $$\n",
    "\n",
    "**Hint:** Make clever use of `div` (Python operator `//`) and `modulo` (Python operator `%`) to extract the individual digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "13714c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_digits(x):\n",
    "    if x<10:\n",
    "        return x\n",
    "    else:\n",
    "        return x%10+sum_digits(x//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e4ce6232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_long (__main__.SumDigitsTest) ... ok\n",
      "test_one (__main__.SumDigitsTest) ... ok\n",
      "test_small (__main__.SumDigitsTest) ... ok\n",
      "test_zero (__main__.SumDigitsTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x204e63aa880>"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tests\n",
    "unittest.main(argv=['ignored', '-v', 'SumDigitsTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adcf2a",
   "metadata": {},
   "source": [
    "### b) Levenshtein Distance (3 Points)\n",
    "Implement a function `lev(a,b)` that returns the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between the two strings `a` and `b`. The Levenshtein distance is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{lev}(a,b) =\n",
    "\\begin{cases}\n",
    "|a| & \\text{if}\\ |b| = 0\\\\\n",
    "|b| & \\text{if}\\ |a| = 0\\\\\n",
    "\\text{lev}(\\text{tail}(a), \\text{tail}(b)) & \\text{if}\\ a[0] = b[0]\\\\\n",
    "1 + \\text{min}(\\text{lev}(\\text{tail}(a), b), \\text{lev}(a, \\text{tail}(b)), \\text{lev}(\\text{tail}(a), \\text{tail}(b))) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "where:\n",
    "* |a| represents the length of the string _a_\n",
    "* a[0] is the first character of the string _a_\n",
    "* tail(a) is a string of all but the first character of _a_\n",
    "\n",
    "Note: The straightforward recursive implementation is highly inefficient. Therefore, only small strings (up to ~10 characters) should be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b38fe24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail(x):\n",
    "    return x[1:]\n",
    "\n",
    "def lev(s1, s2):\n",
    "    if len(s1) == 0: return len(s2)\n",
    "    if len(s2) == 0: return len(s1)\n",
    "    if s1[-1] == s2[-1]: cost = 0\n",
    "    else: cost = 1\n",
    "    res = min([lev(s1[:-1], s2)+1, lev(s1, s2[:-1])+1, lev(s1[:-1], s2[:-1]) + cost])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0e0ace3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_equal (__main__.LevenshteinDistanceTest) ... ok\n",
      "test_long (__main__.LevenshteinDistanceTest) ... ok\n",
      "test_normal (__main__.LevenshteinDistanceTest) ... ok\n",
      "test_small (__main__.LevenshteinDistanceTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.707s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x204e63ae790>"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tests\n",
    "unittest.main(argv=['ignored', '-v', 'LevenshteinDistanceTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794002e",
   "metadata": {},
   "source": [
    "## Exercise 3: Classes (5 Points)\n",
    "\n",
    "In this exercise, we will implement a small movie company structure consisting of `Movie`s and `Person`s.\n",
    "\n",
    "The `Movie` class should contain the following data attributes.\n",
    "* `title` - the movie title\n",
    "* `year` - the year in which the movie was released\n",
    "* `genre` - the main genre of the movie, one of [ACTION, DRAMA, THRILLER, COMEDY, HORROR]\n",
    "\n",
    "*Note: We expect the genre to be all in upper case, therefore, e.g. \"Action\" or \"action\" are invalid genres.*\n",
    "\n",
    "The `Movie` class should contain the following methods.\n",
    "* `__init__(self, title, year, genre)`: the constructor taking three arguments $title$, $year$, and $genre$ representing the title, year, and genre; the constructor should check the correctness of the `genre` parameter and print a warning in case a incorrect value was provided (Note: the class will still be created)\n",
    "* `get_title(self)` - returns the `title` of the movie\n",
    "* `get_year(self)` - returns the `year` of the movie\n",
    "* `get_genre(self)` - returns the `genre` of the movie\n",
    "* `__eq__(self, other)` - returns `True` iff the `title`, `year`, and `genre` are equal, `False` otherwise.\n",
    "\n",
    "*Hint: The function `__eq__` overloads the equality operator `==`. If `__eq__` is defined on `x` the expression `x == y` is the same as `x.__eq__(y)`. This allows you to check if two movies `x` and `y` are equal by `x == y`. For more information see [here](https://docs.python.org/3/reference/datamodel.html#object.__eq__).*\n",
    "\n",
    "A `Person` has a `name` and a `surname`. Furthermore, a `Person` can either be an `Actor` or a `Director`, which should be represented using inheritance. A `Director` has exactly one `Movie` that she or he directed. An `Actor` contains a list of `Movie`s she or he participated in.\n",
    "\n",
    "The `Person` class should contain the following methods.\n",
    "* `get_name(self)` - returns the name of the person\n",
    "* `get_surname(self)` - returns the surname of the person\n",
    "\n",
    "The `Director` class should contain the following methods.\n",
    "* `get_movie(self)` - returns the `Movie` directed by the `Director`\n",
    "* `actor_in_movie(self, actor)` - takes an `Actor` and returns `True` iff the `Actor` participated in the `Director`'s `Movie`, `False` otherwise\n",
    "\n",
    "The `Actor` class should contain the following methods.\n",
    "* `get_all_movies(self)` - returns the list of **all** `Movie`s the `Actor` participated in\n",
    "* `get_movies(self, year, genre)` - returns a list containing the `Movies` with the genre `genre` and were released **after** `year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "789e60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0c57b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_movie_ctor (__main__.MovieTest) ... ERROR\n",
      "test_movie_eq (__main__.MovieTest) ... ERROR\n",
      "test_movie_get_genre (__main__.MovieTest) ... ERROR\n",
      "test_movie_get_title (__main__.MovieTest) ... ERROR\n",
      "test_movie_get_year (__main__.MovieTest) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_movie_ctor (__main__.MovieTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 130, in test_movie_ctor\n",
      "    m = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_movie_eq (__main__.MovieTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 154, in test_movie_eq\n",
      "    m = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_movie_get_genre (__main__.MovieTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 149, in test_movie_get_genre\n",
      "    m = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_movie_get_title (__main__.MovieTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 139, in test_movie_get_title\n",
      "    m = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_movie_get_year (__main__.MovieTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 144, in test_movie_get_year\n",
      "    m = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.007s\n",
      "\n",
      "FAILED (errors=5)\n",
      "test_actor_ctor (__main__.PersonTest) ... ERROR\n",
      "test_actor_get_all_movies (__main__.PersonTest) ... ERROR\n",
      "test_actor_get_movies_contrained (__main__.PersonTest) ... ERROR\n",
      "test_actor_inheritance (__main__.PersonTest) ... ERROR\n",
      "test_director_actor_in_movie (__main__.PersonTest) ... ERROR\n",
      "test_director_actor_not_in_movie (__main__.PersonTest) ... ERROR\n",
      "test_director_ctor (__main__.PersonTest) ... ERROR\n",
      "test_director_get_movie (__main__.PersonTest) ... ERROR\n",
      "test_director_inheritance (__main__.PersonTest) ... ERROR\n",
      "test_person_ctor (__main__.PersonTest) ... ERROR\n",
      "test_person_get_name (__main__.PersonTest) ... ERROR\n",
      "test_person_get_surname (__main__.PersonTest) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_actor_ctor (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 219, in test_actor_ctor\n",
      "    m_avatar = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_actor_get_all_movies (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 226, in test_actor_get_all_movies\n",
      "    m_avatar = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_actor_get_movies_contrained (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 233, in test_actor_get_movies_contrained\n",
      "    m_avatar = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_actor_inheritance (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 241, in test_actor_inheritance\n",
      "    self.assertTrue(issubclass(Actor, Person))\n",
      "NameError: name 'Actor' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_director_actor_in_movie (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 203, in test_director_actor_in_movie\n",
      "    m_avatar = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_director_actor_not_in_movie (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 211, in test_director_actor_not_in_movie\n",
      "    m_avatar = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_director_ctor (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 184, in test_director_ctor\n",
      "    m = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_director_get_movie (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 197, in test_director_get_movie\n",
      "    m = Movie(\"Avatar\", 2009, \"ACTION\")\n",
      "NameError: name 'Movie' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_director_inheritance (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 194, in test_director_inheritance\n",
      "    self.assertTrue(issubclass(Director, Person))\n",
      "NameError: name 'Director' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_person_ctor (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 167, in test_person_ctor\n",
      "    p = Person(\"Erika\", \"Mustermann\")\n",
      "NameError: name 'Person' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_person_get_name (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 174, in test_person_get_name\n",
      "    p = Person(\"Erika\", \"Mustermann\")\n",
      "NameError: name 'Person' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_person_get_surname (__main__.PersonTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 179, in test_person_get_surname\n",
      "    p = Person(\"Erika\", \"Mustermann\")\n",
      "NameError: name 'Person' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.010s\n",
      "\n",
      "FAILED (errors=12)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x204e63345b0>"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tests\n",
    "unittest.main(argv=['ignored', '-v', 'MovieTest'], verbosity=2, exit=False)\n",
    "unittest.main(argv=['ignored', '-v', 'PersonTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7631765",
   "metadata": {},
   "source": [
    "## Exercise 4: Data Cleaning (6 Points)\n",
    "\n",
    "In this exercise, we will again deal with the [IMDb](https://www.imdb.com/interfaces/) dataset from the lecture. However, this time it has been slightly changed, so some data needs to be corrected. First we have to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "011f9696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                     Title                     Genre              Director  \\\n0  Guardians of the Galaxy   Action,Adventure,Sci-Fi            James Gunn   \n1               Prometheus  Adventure,Mystery,Sci-Fi          Ridley Scott   \n2                    Split           Horror,Thriller    M. Night Shyamalan   \n3                     Sing   Animation,Comedy,Family  Christophe Lourdelet   \n4            Suicide Squad  Action,Adventure,Fantasy            David Ayer   \n\n                                              Actors    Year  Rating  \n0  Chris Pratt, Vin Diesel, Bradley Cooper, Zoe S...  2014.0     8.1  \n1  Noomi Rapace, Logan Marshall-Green, Michael Fa...  2012.0     7.0  \n2  James McAvoy, Anya Taylor-Joy, Haley Lu Richar...  2016.0     7.3  \n3  Matthew McConaughey,Reese Witherspoon, Seth Ma...  2016.0     7.2  \n4  Will Smith, Jared Leto, Margot Robbie, Viola D...  2016.0     6.2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Genre</th>\n      <th>Director</th>\n      <th>Actors</th>\n      <th>Year</th>\n      <th>Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Guardians of the Galaxy</td>\n      <td>Action,Adventure,Sci-Fi</td>\n      <td>James Gunn</td>\n      <td>Chris Pratt, Vin Diesel, Bradley Cooper, Zoe S...</td>\n      <td>2014.0</td>\n      <td>8.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Prometheus</td>\n      <td>Adventure,Mystery,Sci-Fi</td>\n      <td>Ridley Scott</td>\n      <td>Noomi Rapace, Logan Marshall-Green, Michael Fa...</td>\n      <td>2012.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Split</td>\n      <td>Horror,Thriller</td>\n      <td>M. Night Shyamalan</td>\n      <td>James McAvoy, Anya Taylor-Joy, Haley Lu Richar...</td>\n      <td>2016.0</td>\n      <td>7.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sing</td>\n      <td>Animation,Comedy,Family</td>\n      <td>Christophe Lourdelet</td>\n      <td>Matthew McConaughey,Reese Witherspoon, Seth Ma...</td>\n      <td>2016.0</td>\n      <td>7.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Suicide Squad</td>\n      <td>Action,Adventure,Fantasy</td>\n      <td>David Ayer</td>\n      <td>Will Smith, Jared Leto, Margot Robbie, Viola D...</td>\n      <td>2016.0</td>\n      <td>6.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(1003, 6)"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('imdb_corrupt.csv', delimiter=';')\n",
    "\n",
    "# Print infos\n",
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525b78a",
   "metadata": {},
   "source": [
    "### a) Eliminate duplicates (3 Points)\n",
    "\n",
    "One reason for duplicates can be spelling errors or inconsistent usage of (superfluous) special characters or whitespaces. In this exercise, we will make use of the well know edit distance [Hammming distance](https://en.wikipedia.org/wiki/Hamming_distance) to identify duplicate entries due to spelling errors in the movie titles.\n",
    "\n",
    "*Note: This only removes one potential source of error.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b8285",
   "metadata": {},
   "source": [
    "1. Implement a function `hamming_distance_custom(a, b)`, that takes two strings `a` and `b` and returns the hamming distance of both strings. Our custom Hamming distance is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{hamming_distance_custom}(a,b) =\n",
    "\\begin{cases}\n",
    "\\infty & \\text{if}\\ |a| \\neq |b|\\\\\n",
    "0 & \\text{if}\\ |a| =  0\\\\\n",
    "0 + \\text{hamming_distance_custom}(\\text{tail}(a), \\text{tail}(b)) & \\text{if}\\ a[0] = b[0]\\\\\n",
    "1 + \\text{hamming_distance_custom}(\\text{tail}(a), \\text{tail}(b)) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "* |a| represents the length of the string _a_\n",
    "* a[0] is the first character of the string _a_\n",
    "* tail(a) is a string of all but the first character of _a_\n",
    "\n",
    "Note: You can use `math.inf` as $\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d21aee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "70578c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_different_length (__main__.HammingDistanceTest) ... ERROR\n",
      "test_empty (__main__.HammingDistanceTest) ... ERROR\n",
      "test_equal (__main__.HammingDistanceTest) ... ERROR\n",
      "test_long (__main__.HammingDistanceTest) ... ERROR\n",
      "test_normal (__main__.HammingDistanceTest) ... ERROR\n",
      "test_small (__main__.HammingDistanceTest) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_different_length (__main__.HammingDistanceTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 275, in test_different_length\n",
      "    res = hamming_distance_custom(a, b)\n",
      "NameError: name 'hamming_distance_custom' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_empty (__main__.HammingDistanceTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 282, in test_empty\n",
      "    res = hamming_distance_custom(a, b)\n",
      "NameError: name 'hamming_distance_custom' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_equal (__main__.HammingDistanceTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 261, in test_equal\n",
      "    res = hamming_distance_custom(a, b)\n",
      "NameError: name 'hamming_distance_custom' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_long (__main__.HammingDistanceTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 268, in test_long\n",
      "    res = hamming_distance_custom(a, b)\n",
      "NameError: name 'hamming_distance_custom' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_normal (__main__.HammingDistanceTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 254, in test_normal\n",
      "    res = hamming_distance_custom(a, b)\n",
      "NameError: name 'hamming_distance_custom' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_small (__main__.HammingDistanceTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 247, in test_small\n",
      "    res = hamming_distance_custom(a, b)\n",
      "NameError: name 'hamming_distance_custom' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.007s\n",
      "\n",
      "FAILED (errors=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x204e63762b0>"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tests\n",
    "unittest.main(argv=['ignored', '-v', 'HammingDistanceTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab1d97",
   "metadata": {},
   "source": [
    "2. Check if the movies given in `movies_to_check` contain any spelling mistakes. To do so, iterate over the dataframe `df` and compute the pairwise hamming distance between each title and the movie titles given in `movies_to_check`. If the hamming distance is between 0 and 3 (exclusive, i.e. we have up to two typos), add the potentially misspelled movie title to the `duplicate_movies` list.\n",
    "\n",
    "*Note: This method only identifies potential spelling mistakes. There might be errors that are not detected (e.g. if a character is missing) or very similar movie titles that are detected but in reality, do not contain any spelling mistakes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "825d523a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies_to_check = ['Harry Potter and the Deathly Hallows: Part 1',\n",
    "                   'Star Wars: Episode VII',\n",
    "                   'The Great Wall',\n",
    "                   'Nine Lives',\n",
    "                   'The Lone Ranger',\n",
    "                   'The Lego Movie',\n",
    "                   'Frozen',\n",
    "                   'Lucky Number Slevin'\n",
    "                  ]\n",
    "\n",
    "duplicate_movies = []\n",
    "# TODO: implement\n",
    "            \n",
    "display(duplicate_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f2463f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_duplicates (__main__.DuplicatesTest) ... FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_duplicates (__main__.DuplicatesTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\uds zeinab\\EDSAI\\Sheet1\\assignment01_unittests.py\", line 290, in test_duplicates\n",
      "    self.assertCountEqual(duplicate_movies, expected_duplicates)\n",
      "AssertionError: Element counts were not equal:\n",
      "First has 0, Second has 1:  'Harry Potter and the Deathly Hallows: Part 2'\n",
      "First has 0, Second has 1:  'The Graet Wall'\n",
      "First has 0, Second has 1:  'Frozem'\n",
      "First has 0, Second has 1:  'The Lego Movoe'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x204e633fd90>"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tests\n",
    "unittest.main(argv=['ignored', '-v', 'DuplicatesTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdde50b",
   "metadata": {},
   "source": [
    "3. Afterwards, manually inspect the list of potentially duplicated movies and remove those movies, that actually do not contain any spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "833a5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ecbe0",
   "metadata": {},
   "source": [
    "4. Finally, we remove all duplicates from the DataFrame `df` and store the result in the `df` variable again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "59f2dbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [Title, Genre, Director, Actors, Year, Rating]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Genre</th>\n      <th>Director</th>\n      <th>Actors</th>\n      <th>Year</th>\n      <th>Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(1003, 6)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_duplicates = df[(df['Title'].isin(duplicate_movies))]\n",
    "display(df_duplicates)\n",
    "df.drop(df_duplicates.index, inplace=True, axis='index')\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91e7d3",
   "metadata": {},
   "source": [
    "### b) Replace missing and erroneous values (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7d45c",
   "metadata": {},
   "source": [
    "1. Missing values are denoted as `NaN` in `Pandas`. List all rows that contain missing values and store them in a new DataFrame `df_missing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "47dc0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dcc5b25e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_missing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [157], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m display(\u001B[43mdf_missing\u001B[49m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Run tests\u001B[39;00m\n\u001B[0;32m      3\u001B[0m unittest\u001B[38;5;241m.\u001B[39mmain(argv\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignored\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-v\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissingTest\u001B[39m\u001B[38;5;124m'\u001B[39m], verbosity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, exit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_missing' is not defined"
     ]
    }
   ],
   "source": [
    "display(df_missing)\n",
    "# Run tests\n",
    "unittest.main(argv=['ignored', '-v', 'MissingTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97feea",
   "metadata": {},
   "source": [
    "Afterwards, check [IMDB.com](https://www.imdb.com/) for the entries with missing values and manually add them to the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8aaecb",
   "metadata": {},
   "source": [
    "2. Erroneous values\n",
    "\n",
    "In the following, we want to examine the attributes `Year` and `Rating` for potential erroneous values. Here, the rating is a number between 0 and 10 (inclusive) and the data covers the years from 2006 to 2016. A good approach to find erroneous values in numerical data is to visualize them. Therefore, create boxplots for the columns in question using `matplotlib` (see also [here](https://stackoverflow.com/questions/17725927/boxplots-in-matplotlib-markers-and-outliers))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbf6fb",
   "metadata": {},
   "source": [
    "Through the plot outliers become visible, which give an indication of possibly erroneous values. Filter potential outlier values appropriately and store them in the DataFrame `df_outliers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a76e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_outliers)\n",
    "# Run tests\n",
    "unittest.main(argv=['ignored', '-v', 'OutliersTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ceba80",
   "metadata": {},
   "source": [
    "Again, check [IMDB.com](https://www.imdb.com/) for the entries with potential erroneous values and manually correct them in the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
